A work in progress


# Implementing a New Tuning Strategy

This document assumes familiarity with the [Evaluating Model
Performance](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/) and [Performance
  measures](https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/)
sections of the MLJ manual.

### Overview 

What follows is an overview of tuning in MLJ. After the overview is an
elaboration on those terms given in *italics*.

All tuning in MLJ is conceptualized as an iterative procedure, each
iteration corresponding to a performance *evaluation* of a single
*model* instance. Each such instance is a mutation of a fixed
*prototype*. In the general case, this prototype is a composite model,
i.e., a model with other models as hyperparameters, and the parameters
these sub-models are allowed to change.

When all iterations of the algorithm are complete, the optimal model
is selected based entirely on a *history* generated according to the
specified *tuning strategy*. Iterations are generally performed in
batches, which are evaluated in parallel (sequential tuning strategies
degenerating into semi-sequential strategies, unless the batch size is
one). At the beginning of each batch, both the history and an internal
*state* object are consulted, and, on the basis of the tuning
strategy, a new batch of models to be evaluated is generated. On the
basis of these evaluations, and the strategy, the history and internal
state are updated.

The tuning algorithm initializes the state object before iterations
begin, on the basis of the specific strategy and a user-specified
*range* object.

- Recall that in MLJ a *model* is an object storing the
  hyperparameters of some learning algorithm indicated the model type
  name (e.g., `DecisionTreeRegressor`). Models do not store learned
  parameters.
- An *evaluation* is the value returned by some call to the
  `evaluate!` method, when passed the resampling strategy and
  performance measures specified by the user when specifying the
  tuning task. Recall that such a value is a named tuple of vectors
  with keys `measure`, `measurement`, `per_fold`, and
  `per_observation`. See [Evaluating Model
  Performance](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/)
  for details. Recall also that some measures in MLJ (e.g.,
  `cross_entropy`) report a loss (or score) for each provided
  observation, while others (e.g., `auc`) report only an aggregated
  value. This and other behaviour can be inspected via trait functions. Do
  `info(rms)` to view the traits of the `rms` loss, and see
  [Performance
  measures](https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/)
  for details.
    
- The *history* is a vector of tuples generated by the tuning
  algorithm - one tuple per iteration - used to determine the optimal
  model and which also records other statistics of potential interest
  to the user - for example, evaluations of a measure (loss or score)
  different from one being explicitly optimized. Each tuple is of the
  form `(m, r)`, where `m` is a model instance and `r` is information
  about `m` extracted from an evaluation.   

- A *tuning strategy* is an instance of some subtype `S <:
  TuningStrategy`, the name `S` (e.g., `Grid`) indicating the tuning
  algorithm to be applied. The fields of the tuning strategy - called
  *hyperparameters* - are those tuning parameters specific to the
  strategy that **do not refer to specific models or specific model
  hyperparameters**. So, for example, a default resolution to be used
  in a grid search is a hyperparameter of `Grid`, but the resolution
  to be applied to a *specific* model hyperparameter (such as the
  maximum depth of a decision tree) is **not**. This latter parameter
  would be part of the user-specified range object.

- A *range* is any object whose specification completes the
  specification of the tuning task, after the prototype, tuning
  strategy, resampling strategy, performance measure(s), and total
  iteration count are given - roughly, the space of models to be
  searched. This definition is intentionally broad and the interface
  places no restriction on the allowed types of this object. As an
  example, the `Grid` tuning strategy type supports the
  one-dimensional `NumericRange` or `NominalRange` objects (these
  types are provided by the MLJBase), or vectors of such objects,
  which are sampled using the `resolution` hyperparameter. However,
  `Grid` also supports a vector of 1D range/resolution pairs.
  
### Interface points for user input

Recall that in MLJ tuning is implemented as a model wrapper. In
setting up a tuning task, the user constructs an instance of the
`TunedModel` wrapper type, which has these principal fields:

- `model`: the prototype model instance mutated during tuning
- `tuning`: the tuning strategy, an instance of a concrete
  `TuningStrategy` subtype, such as `Grid`
- `resampling`: the resampling strategy used for performance
  evaluations, an instance of a concrete `ResamplingStrategy` subtype,
  such as `Holdout` or `CV`
- `measure`: a measure (loss or score) or vector of measures available
  to the tuning algorithm, the first of which is optimized in the
  common case of single-objective tuning strategies
- `range`: as defined above - roughly, the space of models to be searched
- `n`: the number of iterations (number of distinct models to be
  evaluated)
- `acceleration`: the computational resources to be applied (e.g.,
  `CPUProcesses()` for distributed computing and `CPUThreads()` for
  multithreaded processing)
  

### Implementation requirements for new tuning strategies

#### Summary of functions

Five functions are part of the tuning strategy API: 

- `setup`: for initialization of state (compulsory)
- `result`: for building each element of the history 
- `models!`: for generating batches of new models and updating the
  state (*Note:* the history is updated automatically) (compulsory)
- `best`: for extracting the optimal model from the history 
- `tuning_report`: for selecting what to report to the user apart from
  the optimal model 

These are outlined below, after discussing types.


#### The tuning strategy type

Each tuning algorithm must define a subtype of `TuningStrategy` whose
fields are the hyperparameters controlling the strategy that do not
directly refer to models or model hyperparameters. These would
include, for example, the default resolution of a grid search, or the
initial temperature in simulated annealing.

The algorithm implementation must include a keyword constructor with
defaults. Here's an example:

```julia
mutable struct Grid <: TuningStrategy
    resolution::Int
    acceleration::ComputationalResources.AbstractResource
end

# Constructor with keywords
Grid(; resolution=10, acceleration=MLJBase.DEFAULT_RESOURCE[]) = 
    Grid(resolution, acceleration)
```

Here `ParameterName=Union{Symbol,Expr}`


#### Range types

A type definition is required for each range object a tuning strategy
should like to handle. The following range types are provided
out-of-the-box:

- The one-dimensional range types `NumericRange` and `OrdinalRange`
  (subtypes of `ParamRange`)
- `Vector{ParamRange}` for Cartesian products
  
Recall that `OrdinalRange` has a `values` field, while `NominalRange`
has the fields `upper`, `lower`, `scale`, `unit` and `origin`. The
`unit` field specifies a preferred length scale, while `origin` a
preferred "central value". These default to `(upper - lower)/2` and
`(upper + lower)/2`, respectively, in the bounded case (neither `upper
= Inf` nor `lower = -Inf`). The fields can be used to specify, for
example, a sensible clipped Gaussian over an unbounded `NominalRange`.

Both `ParamRange` subtypes have the additional field, `field`, where
dot syntax is used to specify nested fields, as in
`:(atom.max_depth)`. Query the `OrdinalRange` and `NominalRange` doc
strings for further details.


#### The `result` method: For declaring what parts of an evaluation goes into the history 

```julia
MLJBase.result(tuning::MyTuningStrategy, history, e)
```

This method is for extracting from an evaluation `e` of some model `m`
the value of `r` to be recorded in the corresponding tuple `(m, r)` of
the history. The fallback is

```julia
MLJBase.result(tuning, history, e) = (measure=e.measure, measurement=e.measurement)
```

Note this is generally a tuple of *vectors*, since multiple measures
can be specified.

The history must contain everything needed for the `best` method to
determine the optimal model, and everything needed by the
`report_history` method, which generates a report on tuning to the
user (for use in visualization, for example). These methods are
detailed below.


#### The `setup` method: To initialize state 

```julia
state = setup(tuning::MyTuningStrategy, model, range)
```

The `setup` function is for initializing the mutable `state` of the
tuning algorithm (needed, by the algorithm's `models!` method; see
below). The `state` generally stores, at the least, the range or some
processed version thereof. In momentum-based gradient descent, for
example, the state would include the previous hyperparameter
gradients, while in GP Bayesian optimization, it would store the
(evolving) Gaussian processes. 

If a variable is to be reported as part of the user-inspectable
history, then it should be written to the history instead of stored in
state. An example of this might be the `temperature` in simulated
annealing.

The only fallback for `setup` provided by MLJ is one that reduces
one-dimensional `ParamRange` objects to the vector (i.e., Cartesian)
case:

```julia
MLJBase.setup(tuning::TuningStrategy, model, range::ParamRange) =
  [range, ]
```

A tuning strategy must implement a `setup` method for each range
type it is going to support:

```julia 
MLJBase.setup(tuning::MyTuningStrategy, model, range::RangeType1) 
MLJBase.setup(tuning::MyTuningStrategy, model, range::RangeType2) ... 
```


#### The `models!` method: For generating model batches to evaluate

```julia
MLJBase.models!(tuning::MyTuningStrategy, history, state)
```

This is the core method of a new implementation. Given the existing
`history` and `state`, it must return a vector ("batch") of *new*
model instances to be evaluated. Any number of models can be returned
and the evaluations will be performed in parallel (using the mode of
parallelization defined by the `acceleration` field of the
`TunedModel` instance). *An (external) update of the history only
occurs after these evaluations.*

Most sequential tuning strategies will want include the batch size as
a hyperparameter, which they should call `batch_size`, but this is not
a compulsory field of the tuning strategy type.

In a `Grid` tuning strategy, for example, `models!` returns a random
selection of `n - length(history)` models from the grid, so that
`models!` is called only once (in each call to `MLJBase.fit` or
`MLJBase.update`). In a bona fide sequential method which is
generating models randomly (such as simulated annealing), `models!`
might return a single model, or return a small batch of models to make
use of parallelization (the method becoming "semi-sequential" in that
case). In sequential methods that generate new models
deterministically (such as those choosing models that optimize the
expected improvement of a surrogate statistical model) `models!` would
return a single model.

If the tuning algorithm exhausts it's supply of new models (because,
for example, there is only a finite supply) then it returns an empty
vector.


#### The `best` method: To define what constitutes the "optimal model"

```julia
MLJBase.best(tuning::MyTuningStrategy, history)
```

Returns the best model instance, which will be `m` for some pair `(m,
r)` in the history. 

A fallback whose definition is given below may be used, *provided the
fallback for `result` detailed above has not been overloaded*. In this
fallback for `best`, the best model is the one optimizing performance
estimates for the first measure in the `TunedModel` field `measure`:

```julia
function MLJBase.best(tuning, history)
   measurements = [h[2].measurement[1] for h in history]
   measure = history[1].measure[1]
   if orientation(measure) == :score
       measurements = -measurements
   end
   best_index = argmin(measurements)
   return history[best_index][1]
end
```

####  The `tuning_report` method: To build the user-accessible report

As with any model, fitting a `TunedModel` instance generates a
user-accessible report. In the case of tuning, the report is
constructed with this code:

```julia
report = merge((best_model=best_model,),
                tuning_report(tuning, history, state))
```

where `best_model=best(tuning, history)` and `tuning_report` is a
method the implementer may overload. It should return a named
tuple. The fallback is to return the raw history:

```julia
MLJBase.tuning_report(tuning, history) = (history=history,)
```


## Implementation example: Explicit search

The most rudimentary tuning strategy just evaluates every model in a
specified list, such lists constituting the only kind of supported
range. In this special case the prototype is simply ignored. The
fallback implementations for `result`, `best` and `report_history`
suffice.  Here's the complete implementation:

```julia 
    
import MLJBase
    
mutable struct ExplicitSearch <: MLJBase.TuningStrategy 
    shuffle::Bool=false,
    rng::Union{Int,AbstractRNG}=Random.GLOBAL_RNG)
end

function ExplicitSearch(; 
    shuffle::Bool=false,
    rng::Union{Int,AbstractRNG}=Random.GLOBAL_RNG)
    return ExplicitSearch(shuffle, rng)
end

# the initial state is just the provided range (list of models):
MLJBase.setup(tuning::ExplicitSearch, model, range::Vector{<:Supervised}) =
    range

# models! returns all models in the range at once:
MLJBase.models!(tuning::MyTuningStrategy, history, state) = state 

```


## The generic tuning algorithm


```julia
function update_history!(tuning, history, n, resampling_machine, state)
    while length(history) < n
        models_ = models!(tuning, history, K, state)
		if models_ isa AbstractVector
		    models = models_
	    else
		    models = [models_, ]
        end
        Δhistory = []
        # batch processing (TODO: parallelize this!):
        for m in models
            resampling_machine.model = m
            fit!(resampling_machine)
            e = evaluate(resampling_machine)
            r = result(tuned_model.tuning, history, e)
            Δhistory = push!(Δhistory, (m, r))
        end
        history = vcat(history, Δhistory)
    end
end

function MLJBase.fit(tuned_model::TunedModel, verbosity::Integer, X,  y)

    tuning = tuned_model.tuning
    n = tuned_model.n
    batch_size = tuned_model.batch_size
    domain = tuned_model.range
    model = tuned_model.model

    # omitted: checks that measures are appropriate

    state = setup(tuning, model, range)

    # instantiate resampler (`model` to be replaced with mutated
    # clones during iteration below):
    resampler = Resampler(model=model,
                          resampling = tuned_model.resampling,
                          measure    = tuned_model.measure,
                          weights    = tuned_model.weights,
                          operation  = tuned_model.operation)
    resampling_machine = machine(resampler, X, y)

    history = []
    update_history!(tuning, history, n, resampling_machine, state)

    best_model = best(tuning, history)

    fitresult = machine(best_model, X, y)
    fit!(fitresult, verbosity=verbosity-1)

    report = merge((best_model=best_model,),
                    tuning_report(tuning, history, state))

    meta_state = (history, deepcopy(tuned_model), state)

    return fitresult, report, stuff
end

function MLJBase.update(tuned_model::TunedModel, verbosity::Integer,
                        old_fitresult, old_meta_state, X, y)

    history, old_tuned_model, state = old_meta_state

    n = tuned_model.n

    if isequal_except(tuned_model, old_tuned_model, :n)

        if tuned_model.n > old_tuned_model.n
            tuned_model.n = n - old_model.n # temporarily mutate tuned_model
            update_history!(tuning, history, n, resampling_machine, state)
            tuned_model.n = n # restore tuned_model to original state

        else
            verbosity < 1 || @info "Number of tuning iterations `n` lowered.\n"*
            "Truncating existing tuning history and retraining new best model."
        end
        best_model = best(tuning, history)

        fitresult = machine(best_model, X, y)
        fit!(fitresult, verbosity=verbosity - 1)

        report = merge((best_model=best_model,),
                       tuning_report(tuning, history, state))

        meta_state = (history, deepcopy(tuned_model), state)

        return fitresult, report, meta_state

    else

        return fit(tuned_model, verbosity, X, y)

    end

end

MLJBase.predict(tuned_model::TunedModel, fitresult, Xnew) =
    predict(fitresult, Xnew)
```

*Not implemented above*: User should be able to specify a stream to
which the history is written in each iteration. In any case a
try-catch block should be used to ensure the history is not lost if the
algorithm crashes or is interrupted.

#############

TODO:

- add `unit` and `origin` field to NominalRanges for unbounded case to
  specify a length scale, and add checks on fields.

- define show_as_constructed(::Type{<:TuningStrategy}) = true

- add handling of model exhaustion

- add `range=ranges` kwarg to TunedModel

- sort out how to externally control termination, almost certainly with a model wrapper 
